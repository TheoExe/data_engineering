{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f606da",
   "metadata": {},
   "source": [
    "## ML System Monitoring and Deployment \n",
    "### Data Engineering\n",
    "\n",
    "Last updated: September 29, 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393f7c7",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "- Designing Machine Learning Systems, Chip Huyen\n",
    "- Solution Architect's Handbook, 2nd Edition. Saurabh Shrivastava and Neelanjali Srivastav\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12e45c",
   "metadata": {},
   "source": [
    "### Concepts\n",
    "\n",
    "- model degradation\n",
    "- monitoring vs observability\n",
    "- software failures vs ML failures\n",
    "- data distribution shifts\n",
    "- edge cases\n",
    "- detecting performance issues\n",
    "- performance monitoring plan\n",
    "- deployment strategies including: blue-green deployment, red-black deployment, canary release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146481f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f029364",
   "metadata": {},
   "source": [
    "### 1. Model Degradation\n",
    "Model performance inevitably degrades over time in production\n",
    "\n",
    "Several reasons for this, some **software** related and some **ML** related\n",
    "\n",
    "**Software failures** include:  \n",
    "- dependency issue: the software changes, vanishes\n",
    "- deployment issue: wrong version deployed, not deployed to correct machine(s)\n",
    "- hardware issue\n",
    "\n",
    "**ML failures** include:  \n",
    "- training data distribution differs from production (inference) data distribution\n",
    "- edge cases\n",
    "\n",
    "Next, we dive deeper on ML failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d2bff",
   "metadata": {},
   "source": [
    "#### A. Training data distribution differs from production (inference) data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155989e",
   "metadata": {},
   "source": [
    "ML works well when patterns in data at production time match patterns in data at training time.  \n",
    "This is generalization.\n",
    "\n",
    "Several reasons why this might fail to be the case:\n",
    "\n",
    "- **non-stationarity**: patterns change over time for various reasons:\n",
    "    - major disruption like pandemic\n",
    "    - seasonality\n",
    "    - change in economic / market conditions\n",
    "    - change in business strategy\n",
    "\n",
    "---\n",
    "\n",
    "**THINK ABOUT AND DISCUSS**\n",
    "\n",
    "Can you think of other reasons why the pattern might change?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- **change in feature cardinality**: credit score range changes from 300-850 to 300-830\n",
    "\n",
    "- **bad data** including:\n",
    "  - incorrect inputs\n",
    "  - unexpected data format\n",
    "  - issue with data collection / pipeline  \n",
    " \n",
    "Change is common\n",
    "  \n",
    "Can often be hard to detect as ML issues can faily silently.\n",
    "\n",
    "Often need to run statistical test (e.g., two-sample test) to detect significant change.\n",
    "\n",
    "Popular non-parameteric test: Kolmogorov-Smirov (KS) test of two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b6017",
   "metadata": {},
   "source": [
    "**Retraining the model**\n",
    "\n",
    "A common practice for dealing w changes in data over time: retrain the model.\n",
    "\n",
    "Alternative terminology: fine tune model, recalibrate model\n",
    "\n",
    "This keeps architecture and features the same but changes the data\n",
    "\n",
    "Some considerations when retraining:\n",
    "\n",
    "- what time period to use? \n",
    "\n",
    "- expanding or sliding window?\n",
    "  \n",
    "- how often to retrain?\n",
    "\n",
    "Run tests to decide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9936e9",
   "metadata": {},
   "source": [
    "#### Edge Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b04b15",
   "metadata": {},
   "source": [
    "This is situation where model performs poorly.\n",
    "\n",
    "Example: Model trained on financial data when interest rates were always positive.  \n",
    "In production, it is fed negative interest rates. This might produce poor results.\n",
    "\n",
    "**Helps to include edge cases in training data to make more robust.**\n",
    "\n",
    "Next, we talk about how to detect pattern changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a3707",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d016b89",
   "metadata": {},
   "source": [
    "### 2. Monitoring and Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f96d26",
   "metadata": {},
   "source": [
    "*Monitoring* refers to the act of tracking, measuring and logging different metrics to help determine when something goes wrong.\n",
    "\n",
    "*Observability* refers to setting up the system so that users have **visibility into the system** to determine when something goes wrong and where it happened. An example would be logging all events in the system as it runs. Sometimes called *instrumentation*.\n",
    "\n",
    "Observability should allow drill down. For example: user wants to see all incorrect predictions for certain subset of customers over certain period of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7d82c",
   "metadata": {},
   "source": [
    "#### What Should be Monitored?\n",
    "\n",
    "**Operational Metrics**  \n",
    "Examples may include:\n",
    "- latency: time elapsed between request and returned answer\n",
    "- throughput: amount of data processed over given period\n",
    "- CPU/GPU utilization\n",
    "- memory utilization\n",
    "- number of prediction requests received over given period\n",
    "- uptime: percent of time that system is available to offer reasonable performance\n",
    "\n",
    "Uptime example: At one time AWS EC2 offered monthly uptime percentage of at least 99.99% (four nines).\n",
    "If the monthly uptime percentage fell below this level, they would give a credit.\n",
    "\n",
    "**ML-Specific Metrics**  \n",
    "Broad categories to monitor:\n",
    "- model accuracy (very important)\n",
    "- predictions (very important)\n",
    "- features\n",
    "- raw inputs\n",
    "\n",
    "Since model degradation is the focus, it's most important to monitor model accuracy and predictions.  \n",
    "Features may change in distribution, but if the model continues to perform well, this is not concerning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c73950",
   "metadata": {},
   "source": [
    "For monitoring **model accuracy**, examples include:\n",
    "- accuracy\n",
    "- F1 score\n",
    "- area under the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fef9b",
   "metadata": {},
   "source": [
    "For **monitoring predictions**, examples include:\n",
    "- any invalid predicted probabilities: less than 0 or greater than 1?\n",
    "- have all predictions over some period of time been identical? this would be worrisome.\n",
    "- Run test cases with known answer. Does the prediction vary over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7facab",
   "metadata": {},
   "source": [
    "For **monitoring features**, examples include:\n",
    "- statistics of each feature (quantiles, median, ...)\n",
    "- do values fall within expected range (for continuous values)\n",
    "- do values fall within predefined set (for discrete values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f343f88",
   "metadata": {},
   "source": [
    "For monitoring **raw inputs**, examples include:\n",
    "- checking for missing data (example: your system scrapes web pages and the format has changed, returning no data)\n",
    "- checking in invalid data formats (example: your system expects numeric but is capturing text)\n",
    "- data falls outside expected ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f8c653",
   "metadata": {},
   "source": [
    "**Visualizations** can be produced over time. These can be helpful for human review and further exploration, but aren't as useful in automated alerting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc0f7a",
   "metadata": {},
   "source": [
    "#### Performance Monitoring Plan\n",
    "\n",
    "A **performance monitoring plan** is recommended for each model.  \n",
    "This should be crafted by stakeholders to include:\n",
    "- metrics to monitor\n",
    "- triggers for each metric (e.g., if AUC falls by 10% between review periods, then ALERT)\n",
    "- monitoring frequency\n",
    "- actions to take if ALERT (who does what by when)\n",
    "\n",
    "Oftentimes there are three status levels such as RED/AMBER/GREEN (RAG Status).  \n",
    "Stakeholders would define what each level means and what should be done.\n",
    "\n",
    "Simple Example:\n",
    "\n",
    "| LEVEL      | METRIC | ACTION |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| GREEN      | AUROC>=0.8 | system functioning as expected       |\n",
    "| AMBER   | 0.7<AUROC<0.8 | system not functioning as expected but not critical; retrain the model on new data and monitor closely        |\n",
    "| RED   |  AUROC<=0.7| system not functioning as expected and critical; cease use of model and redevelop. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c8606c",
   "metadata": {},
   "source": [
    "### 3. Monitoring Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279bc5e",
   "metadata": {},
   "source": [
    "We summarize useful monitoring tools:\n",
    "\n",
    "- logs\n",
    "- dashboards\n",
    "- alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01565f",
   "metadata": {},
   "source": [
    "**Logs** capture anything of interest. Ideally they include the process ID and metadata to easily track down issues. This can be hard as systems grow complex with multiple microservices running.\n",
    "\n",
    "Can process logs in batch processes to find issues. Spark can be a useful tool.\n",
    "\n",
    "Can look for anomalies in real time using a tool like Kafka or Amazon Kinesis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7dec39",
   "metadata": {},
   "source": [
    "**Dashboards** can be helpful for visualizing all important metrics in one place. Useful for non-technical audience as well (e.g., executives.\n",
    "\n",
    "Include only important metrics and graphs.\n",
    "\n",
    "Powerful tools include Tableau and Power BI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f3e4b",
   "metadata": {},
   "source": [
    "**Alerts** are useful for engaging the right people when the system malfunctions.  \n",
    "This was discussed above in the performance monitoring plan.  \n",
    "\n",
    "Alerts consist of:\n",
    "- conditions when to alert (AUC falls by 10%, AUC < 0.8, ...)\n",
    "- who to alert and how (notify MLOps Team by email, Slack, ...)\n",
    "- description of the alert\n",
    "\n",
    "It's important that alerts are accurate.\n",
    "\n",
    "Tools for setting and monitoring alerts include:\n",
    "- Amazon CloudWatch\n",
    "- GCP Cloud Monitoring\n",
    "- [Datadog](https://www.datadoghq.com/)\n",
    "\n",
    "Some firms create customized jobs that run monitoring\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db747087-ee79-4315-b721-ed4d0165eb3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Deployment Strategies\n",
    "\n",
    "As part of continuous deployment (CD), we'll want to carefully release updates to the system.  \n",
    "Doing a full release to all users, while faster, carries greater risk.\n",
    "\n",
    "In practice, different rollout patterns are used, including:\n",
    "\n",
    "- **Red-black deployment**  \n",
    "  This is the full release: instant cutover from the existing version to the new version.  \n",
    "  While the new software gets out faster, if there are any issues, **all active users** might get exposed to them.  \n",
    "  \n",
    "    In practice, red-black deployment gets combined with **canary testing**.  \n",
    "    It works like this:  \n",
    "\n",
    "  1. Stand up a copy of the environment with the new code (the canary)\n",
    "  2. Route 1% of production traffic to the canary\n",
    "  3. If the canary clears the test, do the cutover\n",
    "  4. If the canary fails, fix the error(s) and rety  \n",
    "\n",
    "\n",
    "- **Blue-green deployment**:  \n",
    "  Gradually replace the existing version with the new version.  \n",
    "  \n",
    "  It works like this:  \n",
    "  1. The BLUE environment carries live traffic on the existing version  \n",
    "  2. You provision a GREEN environment with an identical environment, but with the new code  \n",
    "  3. Route some production traffic from BLUE to GREEN  \n",
    "  4. If there are issue with GREEN, fall back to BLUE. Otherwise route more traffic to GREEN until 100% runs there.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
